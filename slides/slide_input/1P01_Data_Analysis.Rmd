---
title: "Data Analysis"
author: "Brian A. Fannin"
date: "August 21, 2017"
output:
  revealjs::revealjs_presentation:
    center: no
    css: ./css/revealOpts.css
    reveal_plugins:
    - notes
    - zoom
    self_contained: no
    theme: solarized
    transition: slide
  tufte::tufte_handout: default
bibliography: bibliography.bib
---

```{r include=FALSE}
knitr::opts_chunk$set(
    warning=FALSE
  , error=FALSE
  , echo=TRUE
  , message=FALSE
  , fig.height = 4.5
  , collapse = TRUE
)
knitr::opts_knit$set(root.dir = normalizePath('../../'))
```

# Preliminaries

## What we'll cover

* Basic data analysis in R
* Fit a distribution
* Linear regression

## Packages we'll use
  
* `MASS` (MASS = Modern Applied Statistics in S)
    * `fitdistr` will fit a distribution to a loss distribution function
* `actuar` [@actuarPackage]
    * `emm` calculates empirical moments
    * `lev` limited expected value
    * Contains many more distributions than are found in `base` R such as Burr, Pareto, etc. Basically, anything in "Loss Models" is likely to be found here.
    * Contains the dental claims data from "Loss Models"
    
## Generate some loss data

```{r }
set.seed(8910)
years <- 2001:2010
frequency <- 1000

N <- rpois(length(years), frequency)

sevShape <- 2
sevScale <- 1000
severity <- rgamma(sum(N), sevShape, scale = sevScale)
```

# Basic analysis

## Basic exploratory functions

```{r collapse=TRUE}
mean(severity)
median(severity)
var(severity)
sd(severity)
quantile(severity, 0.25)
quantile(severity, c(0.25, 0.5, 0.75))
```

## Summary

```{r}
summary(severity)
```

## Hist again

```{r}
hist(severity)
```

## Density again

```{r}
plot(density(severity))
```

## Basic analysis summary

For univariate continuous data, this is about it. Say, wouldn't it be fun to fit this to a loss distribution?

# Fit a distribution

## fitdistr

```{r }
library(MASS)

fitGamma <- fitdistr(severity, "gamma")
fitLognormal <- fitdistr(severity, "lognormal")
fitWeibull <- fitdistr(severity, "Weibull")

fitGamma
fitLognormal
fitWeibull
```

## q-q plot code

```{r }
probabilities = (1:(sum(N)))/(sum(N)+1)

weibullQ <- qweibull(probabilities, coef(fitWeibull)[1], coef(fitWeibull)[2])
lnQ <- qlnorm(probabilities, coef(fitLognormal)[1], coef(fitLognormal)[2])
gammaQ <- qgamma(probabilities, coef(fitGamma)[1], coef(fitGamma)[2])

sortedSeverity <- sort(severity)
```

## q-q plot, plotting code

```{r eval=FALSE}
oldPar <- par(mfrow = c(1,3))
plot(sort(weibullQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Weibull Fit")
abline(0,1)

plot(sort(lnQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Lognormal Fit")
abline(0,1)

plot(sort(gammaQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Gamma Fit")
abline(0,1)

par(oldPar)
```

## q-q plot

```{r echo=FALSE}
oldPar <- par(mfrow = c(1,3))
plot(sort(weibullQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Weibull Fit")
abline(0,1)

plot(sort(lnQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Lognormal Fit")
abline(0,1)

plot(sort(gammaQ), sortedSeverity, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', pch=19, main = "Gamma Fit")
abline(0,1)

par(oldPar)
```

## Compare fit to histogram

```{r }
sampleLogMean <- fitLognormal$estimate[1]
sampleLogSd <- fitLognormal$estimate[2]

sampleShape <- fitGamma$estimate[1]
sampleRate <- fitGamma$estimate[2]

sampleShapeW <- fitWeibull$estimate[1]
sampleScaleW <- fitWeibull$estimate[2]

x <- seq(0, max(severity), length.out=500)
yLN <- dlnorm(x, sampleLogMean, sampleLogSd)
yGamma <- dgamma(x, sampleShape, sampleRate)
yWeibull <- dweibull(x, sampleShapeW, sampleScaleW)
```

```{r eval=FALSE}
hist(severity, freq=FALSE, ylim=range(yLN, yGamma))

lines(x, yLN, col="blue")
lines(x, yGamma, col="red")
lines(x, yWeibull, col="green")
```

## Compare

```{r echo=FALSE, fig.height=5.5}
hist(severity, freq=FALSE, ylim=range(yLN, yGamma))

lines(x, yLN, col="blue")
lines(x, yGamma, col="red")
lines(x, yWeibull, col="green")
```


## Kolmogorov-Smirnov

The Kolmogorov-Smirnov test measures the distance between a sample distribution's empirical cumulative distribution function and the same for a candidate loss distribution. More formal than q-q plots. 

<!--
## K-S

First, let's visualize that

```{r}
sampleCumul <- seq(1, length(severity)) / length(severity)
stepSample  <- stepfun(sortedSeverity, c(0, sampleCumul), f = 0)
yGamma <- pgamma(sortedSeverity, sampleShape, sampleRate)
yWeibull <- pweibull(sortedSeverity, sampleShapeW, sampleScaleW)
yLN <- plnorm(sortedSeverity, sampleLogMean, sampleLogSd)
```

## Plot K-S

```{r echo=FALSE,  fig.height=5.5}
oldPar <- par(mfrow = c(3,1))
plot(stepSample, col="black", main = "K-S Gamma")
lines(sortedSeverity, yGamma, col = "blue")

plot(stepSample, col="black", main = "K-S Weibull")
lines(sortedSeverity, yWeibull, col = "blue")

plot(stepSample, col="black", main = "K-S Lognormal")
lines(sortedSeverity, yLN, col = "blue")
par(oldPar)
```
-->

## More K-S

```{r }
testGamma <- ks.test(severity, "pgamma", sampleShape, sampleRate)
testLN <- ks.test(severity, "plnorm", sampleLogMean, sampleLogSd)
testWeibull <- ks.test(severity, "pweibull", sampleShapeW, sampleScaleW)

testGamma
testLN
testWeibull
```

A low value for D indicates that the selected curve is fairly close to our data. The p-value indicates the chance that D was produced by the null hypothesis.

## Direct optimization

The `optim` function will optimize a function. Works very similar to the Solver algorithm in Excel. `optim` takes a function as an argument, so let's create a function.

```{r}
quadraticFun <- function(a, b, c){
  function(x) a*x^2 + b*x + c
}

myQuad <- quadraticFun(a=4, b=-3, c=3)
```

## Direct optimization

```{r echo=FALSE}
plot(myQuad, -10, 10)
```

## Direct optimization

8 is our initial guess. A good initial guess will speed up conversion.

```{r }
myResult <- optim(8, myQuad)
myResult
```

## Direct optimization

Default is to minimize. Set the parameter `fnscale` to something negative to convert to a maximization problem.

```{r }
myOtherQuad <- quadraticFun(-6, 20, -5)
plot(myOtherQuad, -10, 10)
```

## Direct optimization

```{r }
myResult <- optim(8, myOtherQuad)
myResult
myResult <- optim(8, myOtherQuad, control = list(fnscale=-1))
myResult
```

## Direct optimization

Direct optimization allows us to create another objective function to maximize, or work with loss distributions for which there isn't yet support in a package like `actuar`. May be used for general purpose optimization problems, e.g. maximize rate of return for various capital allocation methods.

Note that optimization is a general, solved problem. Things like the simplex method already have package solutions in R. You don't need to reinvent the wheel!

# Linear regression

## Fake some data

```{r}
N <- 100
B0 <- 5
B1 <- 1.5
B2 <- 0.8

set.seed(1234)

e <- rnorm(N, mean = 0, sd = 1)
X1 <- runif(N, 0, 10)
X2 <- runif(N, 30, 60)
X3 <- runif(N, 200, 300)

Y <- B0 + B1 * X1 + B2 * X2 + e
```

## First, visualize!

```{r}
plot(X1, Y)
```

## Fit a model

```{r}
fit <- lm(Y ~ X1)
summary(fit)
```

## Formulas

```
y ~ x1
y ~ 1 + x1
y ~ 0 + x1
y ~ x1 + x2
y ~ I(x1 + x2)
y ~ x1 + x2 + x1:x2
```
<aside class="notes">
  Formulas place the response variable on the left and an expression to the right of the `~` character. That character may be read as "is modelled as".
  
  An intercept is implicit. To rermove it, use a "0" or "-1" in the expression
  
  The "+" does not mean addition in the algebraic sense. It means we're adding another predictor variable to our model. To use arithmetic operations, wrap expressions in I()
  
  The : is used for interactions.
</aside>

## Extract data from the fit

| Element | What it shows       |
|------|------------------------|
| residuals | weighted residuals  |
| coefficients  | coefficients    |
| sigma | square root |
| df | degrees of freedom  |
| fstatistic | The F-stat |
| r.squared  | r^2   |
| adj.r.squared  | r^2 penalized for more parameters |
| cov.unscaled  | covariance matrix  |
| correlation   | correlation matrix  |

## Predict

```{r}
prediction <- predict(fit)
```

## Visualize prediction

```{r echo=FALSE}
plot(X1, Y)
lines(X1, prediction, col="red")
```

## Visualize your residuals!

```{r}
plot(Y, residuals(fit))
abline(0, 0, col="red")
```

## What are we looking for?

* Heteroskedasticity => We should change the weights applied to the observations
* Doesn't look like "noise"
    * Serial correlation => Use a time series
    * Apply a transform (polynomial, trig, etc.)
* Extreme values => A normal distribution may not be appropriate

## Try another fit

```{r}
fit2 <- lm(Y ~ X1 + X2)
summary(fit2)

fit3 <- lm(Y ~ X1 + X2 + X3)
summary(fit3)
```

# Questions

## Questions

* Plot a lognormal distribution with a mean of $10,000 and a CV of 30%.
* For that distribution, what is the probability of seeing a claim greater than $100,000?
* Generate 100 and 1,000 observations from that distribution.
* Draw a histogram for each sample.
* What are the mean, standard deviation and CV of each sample?
* Convince yourself that the sample data were not produced by a Weibull distribution.
* Assuming that losses are Poisson distributed, with expected value of 200, estimate the aggregate loss distribution.
* What is the cost of a $50,000 xs $50,000 layer of reinsurance?

## Answers

```{r }
severity <- 10000
CV <- .3
sigma <- sqrt(log(1 + CV^2))
mu <- log(severity) - sigma^2/2
plot(function(x) dlnorm(x), mu, sigma, ylab="LN f(x)")
```

## References
