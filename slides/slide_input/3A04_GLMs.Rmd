---
title: "GLMs"
author: "Brian A. Fannin"
date: "August 23, 2017"
output:
  revealjs::revealjs_presentation:
    center: no
    css: ./css/revealOpts.css
    reveal_plugins:
    - notes
    - zoom
    self_contained: no
    theme: solarized
    transition: slide
  tufte::tufte_handout: default
nocite: |
  @GelmanAndHill
bibliography: bibliography.bib
---

```{r include=FALSE}
knitr::opts_chunk$set(
    warning=FALSE
  , error=FALSE
  , echo=FALSE
  , message=FALSE
  , collapse=TRUE
)
knitr::opts_knit$set(root.dir = normalizePath('../../'))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
```

# Fit a sample

```{r}
set.seed(1234)

num_policies <- 5e3

payroll <- runif(num_policies, 500e3, 10e6)
years_in_operation <- rpois(num_policies, 5)
number_of_employees <- as.integer(runif(num_policies, 50, 2e3))
claims_per_mil <- 1.5 - .1 * years_in_operation + 0.001 * number_of_employees
lambda <- claims_per_mil * payroll / 1e6
claim_count <- rpois(num_policies, lambda)

dfGLM <- data.frame(
    ClaimCount = claim_count
  , Payroll = payroll
  , YearsInOperation = years_in_operation
  , NumberOfEmployees = number_of_employees)

total_claims <- sum(claim_count)
claim_severity <- rgamma(total_claims, 10, 1/10e3)
open_prob <- pmin(claim_severity / 500e3, 1)

dfBinomial <- data.frame(
      ClaimSeverity = claim_severity
    , OpenProb = open_prob
    , Open = rbinom(total_claims, 1, open_prob)
  )
```

## Data 

Claim counts for `r format(num_policies, big.mark = ",")` policies.

```{r fig.height=4}
plt <- ggplot(dfGLM, aes(ClaimCount)) + geom_histogram(binwidth = 1)
plt
```

## How would you fit this data?

## Things we can do when fitting a sample

* Pick a distribution
  * Normal, lognormal, gamma, etc
* Transform data
  * Often taking the log. 
* Pick a fit method
  * Maximum likelihood
  * Least squares
  * Minimum bias
* Assess quality of fit
  * r-squared, penalized r-squared
  * F-stat
  * Likelihood, penalized likelihood

# Add predictors

## Number of claims ~ Payroll

```{r}
plt <- ggplot(dfGLM, aes(Payroll, ClaimCount)) + geom_hex()
plt <- plt + scale_x_continuous(labels = scales::dollar) + ylab("# of Claims")
plt
```

## Number of claims ~ Years in operation

```{r}
plt <- ggplot(dfGLM, aes(as.factor(YearsInOperation), ClaimCount)) + geom_boxplot()
plt <- plt + ylab("# of Claims") + xlab('Years in operation')
plt
```

## Number of claims ~ # of EEs

```{r}
plt <- ggplot(dfGLM, aes(NumberOfEmployees, ClaimCount)) + geom_hex()
plt <- plt + scale_x_continuous(labels = scales::comma)
plt <- plt + ylab("# of Claims") + xlab('Number of Employees')
plt
```

## How about a linear fit?

```{r}
plt <- ggplot(dfGLM, aes(Payroll, ClaimCount)) + geom_hex()
plt <- plt + scale_x_continuous(labels = scales::dollar) + ylab("# of Claims")
plt + geom_smooth(method = "lm", formula = y ~ 1 + x)
```

## What's wrong with this?

* Heteroskedastic
* Does it really capture the mean?

## Another distribution makes more sense

But how do we do that? If only we had a linear model that was a bit more general ... 

# GLMs

## Recall OLS Assumptions

__Warning__: I play fast and loose with the difference between the response variable and the error term and probably lots of other statistical things. I'm not classically trained. I play by ear.

## OLS Assumptions

* Linear relationship between response and predictors: y ~ 1 + x1 + x2
* Errors are normally distributed
* Errors are uncorrelated
* Errors are homoskedastic

## More general assumptions

* Relationship is between response and _transformed_ linear combination of predictors
* Errors need not be normally distributed
* Distributions have some constraints

## Mathematically

$\eta_i=\beta_0+\sum_{j=1}^p\beta_{j}x_{ij}$

$\mu_i= g^{-1}(\eta_i)$

$g(x)$ is the "link" function. I've seen $\eta$ called the systematic component.

I don't know why the expectation is equal to the inverse of the link function. It makes my head hurt.

## Models require us to specify two things

1. The distribution
2. The "link" function

## Distribution restrictions

Must be one of the exponential family of functions.

$f(y; \theta,\phi) = exp[\frac{y\theta - b(\theta)  }{a(\phi)} + c(y,\phi)]$

Note this _doesn't_ include the lognormal. That's OK; we can always do a log transform of our data and fit a normal.

Lots of folks get very excited about this formula. I don't. I can never remember it and I never feel as though I need to. If you like this formula, you'll see it often, but you won't see it any more today.

## Canonical links

| Distribution | Link     |       |
|--------------|----------|-------|
| binomical    | logit    | $g(x)=\frac{exp(x)}{1+exp(x)}$  |
| gaussian     | identity | $g(x)=x$    |
| poisson      | log      | $g(x)=ln(x)$    |
| Gamma        | inverse  | $g(x)=1/x$    |

## Very easy to program

A linear model:

```{r echo=TRUE}
fit_lm <- lm(ClaimCount ~ Payroll, data = dfGLM)
```

A GLM:

```{r echo=TRUE}
fit_glm <- glm(ClaimCount ~ Payroll + YearsInOperation + NumberOfEmployees, data = dfGLM, family = "poisson")
```

## Programmatic differences:

* Must indicate the family
* Must provide the link, though only if we're using something non-canonical

## Summary

```{r}
summary(fit_glm)
```

## Predictions

```{r echo=TRUE}
dfGLM$Predict1 <- predict(fit_glm, type = 'response')
```

# Measuring fit quality

## Measuring fit quality

Comparing models typically involves comparison of the likelihood. Note that - comparable to r^2 - more parameters will _always_ give better fit metrics, unless we're penalizing for extra parameters. AIC and BIC do this. In the formulas below, $p$ is the number of parameters and $L$ is the (conditional) likelihood. Lower is better.

$AIC = 2[-ln(L) + p]$

$BIC = -2L+p*ln(n)$

## Deviance

* Null deviance is similar to sum of squares in OLS. 
* Reduction in residual deviance suggests a better model. Again, adding parameters will _always_ reduce residual deviance. Simple > complex

## Residuals

```{r}
dfGLM$Residual1 <- residuals(fit_glm, type = 'response')
plt <- ggplot(dfGLM, aes(Predict1, Residual1)) + geom_hex()
plt <- plt + geom_hline(yintercept = 0, color = 'red')
plt
```

## Offset

The offset is a kind of scaling factor that should _not_ be included as a predictor. This is comparable to the notion of exposure in insurance pricing. It's difficult to explain and impossible to give fixed guidance about whether you should use as offset. I'll do my best here.

Imagine that you're looking at the number of deaths by heart disease in Manhattan vs. Casper, Wyoming. The number will be higher in Manhattan. Does Manhattan cause death by heart disease? No. (Imagine the CDC telling everyone to move to Wyoming to avoid heart disease.)

## Some numbers

Note that these numbers (apart from population) aren't meant to be realistic. But note that although the _number_ of deaths depends on population, the _rate_ of death does not (in _this_ case).

```{r}
dfHeart <- data.frame(
    City = c("Manhattan", "Casper")
  , Population = c(1.645e6, 59324)
  , AvgAge = c(48, 58)
  , AvgDistanceToHospital = c(0.2, 8)
  , MedianBMI = c(18.7, 28.7))
dfHeart$NumberOfDeaths <- c(5400, 400)
dfHeart$DeathRate <- dfHeart$NumberOfDeaths / (dfHeart$Population/1e6)
```

```{r results='asis'}
knitr::kable(dfHeart)
```

## Compare these two models

Here we'll fit two models. The first will treat payroll as a predictor, the second will treat it as an offset. Fit for the second model is better, because payroll isn't really a _predictor_ of loss. It is a scaling element for exposure.

```{r echo=TRUE, collapse=TRUE}
fit_1 <- glm(ClaimCount ~ 1 + Payroll, data = dfGLM, family="poisson")

fit_2 <- glm(ClaimCount ~ 1, data = dfGLM, family="poisson", offset=log(Payroll))

fit_1$aic
fit_2$aic
coef(fit_1)
coef(fit_2)
```

# Logistic regression (if time permits)

## Binomial

```{r}
plt <- ggplot(dfBinomial, aes(ClaimSeverity, Open)) + geom_hex()
plt <- plt + scale_x_continuous(labels=scales::dollar)
plt
```

## Map predictors to probability

A logistic regression will map a set of predictors to the probability of something. In the prior plot, we can see that the probability of a claim remaining open will increase as the severity goes up.

## The logistic function

Transforms the real number range to a number between zero and one.

$f(\alpha)=\frac{exp(\alpha)}{exp(\alpha)+1}$

## Fitting a logistic

```{r}
fit_open <- glm(Open ~ ClaimSeverity, data=dfBinomial, family="binomial")
summary(fit_open)
```


## Watch that link function!



```{r}
fit_open_1 <- glm(Open ~ 0 + ClaimSeverity, data=dfBinomial, family="binomial")
fit_open_2 <- glm(Open ~ 1 + ClaimSeverity, data=dfBinomial, family="binomial")
summary(fit_open_1)
summary(fit_open_2)
```

##

```{r}
fit_open_1 <- glm(Open ~ 0 + ClaimSeverity, data=dfBinomial, family=binomial(link="identity"))
fit_open_2 <- glm(Open ~ 1 + ClaimSeverity, data=dfBinomial, family=binomial(link="identity"))
summary(fit_open_1)
summary(fit_open_2)
```

## Binomial w/fit

```{r}
plt <- ggplot(dfBinomial, aes(ClaimSeverity, Open)) + geom_hex()
plt <- plt + scale_x_continuous(labels=scales::dollar)
plt + geom_smooth(method = "glm", formula = y ~ 0 + x, method.args = list(family = "binomial"))
```

```{r}
plt <- ggplot(dfBinomial, aes(OpenProb, Open)) + geom_hex()
plt + geom_smooth(method = "glm", formula = y ~ 1 + x, method.args = list(family = "binomial"))
```

